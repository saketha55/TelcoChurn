# -*- coding: utf-8 -*-
"""mera_proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fVyrHzz7yt5Kyq9G6Wa_mPRKvYzw-7TA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.metrics import mean_absolute_error

churn = pd.read_csv('Churn.csv')

churn.head(5)

#churn['PhoneService'].value_counts()
churn_1 = churn.drop(['customerID','MonthlyCharges','TotalCharges'],axis=1)

churn_1.head(3)

#churn_1['PaymentMethod'].unique()

churn_1['MultipleLines'] = churn_1['MultipleLines'].map({'No': 0, 'Yes': 1, 'No phone service': 2})
churn_1['OnlineSecurity'] = churn_1['OnlineSecurity'].map({'No': 0, 'Yes': 1, 'No internet service': 2})
churn_1['OnlineBackup'] = churn_1['OnlineBackup'].map({'No': 0, 'Yes': 1, 'No internet service': 2})
churn_1['DeviceProtection'] = churn_1['DeviceProtection'].map({'No': 0, 'Yes': 1, 'No internet service': 2})
churn_1['TechSupport'] = churn_1['TechSupport'].map({'No': 0, 'Yes': 1, 'No internet service': 2})
churn_1['StreamingTV'] = churn_1['StreamingTV'].map({'No': 0, 'Yes': 1, 'No internet service': 2})
churn_1['StreamingMovies'] = churn_1['StreamingMovies'].map({'No': 0, 'Yes': 1, 'No internet service': 2})

churn_1['Contract'] = churn_1['Contract'].map({'Month-to-month': 0, 'One year': 1, 'Two year': 2})
churn_1['InternetService'] = churn_1['InternetService'].map({'No': 0, 'Fiber optic': 1, 'DSL': 2})

churn_1 = pd.get_dummies(data= churn_1,columns=['gender','Partner','Dependents','PhoneService','PaperlessBilling'],drop_first=True)
churn_1 = pd.get_dummies(data = churn_1, columns=['Churn'],drop_first=True)

"""AFTER ALL THE CHANGES"""

churn_1.head(3)

"""**Differentiating features and labels**"""

# X = features
# Y = label
X = churn_1.drop(['Churn_Yes'], axis=1)
Y = churn_1['Churn_Yes']

X.head()

churn_1.corr()

C_M = churn_1.corr()
plt.figure(figsize=(20,20))
sn.heatmap(C_M, annot= True)
plt.show()

churn_2  = churn_1.drop(columns=['MultipleLines','InternetService','gender_Male','PhoneService_Yes'],axis=1)
churn_2  = churn_2.drop(columns=['SeniorCitizen','Partner_Yes','Dependents_Yes','PaperlessBilling_Yes'],axis=1)
churn_2  = churn_2.drop(columns=['PaymentMethod'],axis=1)
churn_2 = churn_2.dropna()

C_M = churn_2.corr()
plt.figure(figsize=(15,15))
sn.heatmap(C_M, annot=True)
plt.show()

X = churn_2.drop(['Churn_Yes'], axis=1)
Y = churn_2['Churn_Yes']

X['tenure'] = X['tenure'] / X['tenure'].max()
X.head(5)

#5174+1869

#Y.value_counts()

"""K NEAREST NEIGHBOUR CLASSIFICATION"""

from sklearn.model_selection import train_test_split
tr_rs = []
ts_rs = []
for i in range(1,15):
  xtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.3,random_state=i)

  from sklearn.neighbors import KNeighborsClassifier
  kmodel = KNeighborsClassifier(n_neighbors=5)

  kmodel.fit(xtrain,ytrain)

  ytrain_p = kmodel.predict(xtrain)
  tr_rs.append((ytrain_p == ytrain).sum()/len(xtrain))

  ytest_p = kmodel.predict(xtest)
  ts_rs.append((ytest_p == ytest).sum()/len(xtest))

import matplotlib.pyplot as plt
plt.plot(range(1,15),tr_rs)
plt.plot(range(1,15),ts_rs,c='red')
plt.show()

tr_acc = []
ts_acc = []
xtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.3,random_state=3)
for i in range(1,25):
  from sklearn.neighbors import KNeighborsClassifier
  kmodel = KNeighborsClassifier(n_neighbors = i)

  kmodel.fit(xtrain,ytrain)

  ytrain_p = kmodel.predict(xtrain)
  tr_acc.append((ytrain_p == ytrain).sum()/len(xtrain))

  ytest_p = kmodel.predict(xtest)
  ts_acc.append((ytest_p == ytest).sum()/len(xtest))

import matplotlib.pyplot as plt
plt.plot(range(1,25),tr_acc)
plt.plot(range(1,25),ts_acc,c='red')
plt.show()

kmodel = KNeighborsClassifier(n_neighbors = 10)

kmodel.fit(xtrain,ytrain)

ytrain_p = kmodel.predict(xtrain)
print((ytrain_p == ytrain).sum()/len(xtrain))

ytest_p = kmodel.predict(xtest)
print((ytest_p == ytest).sum()/len(xtest))

print(mean_absolute_error(ytrain_p,ytrain))
print(mean_absolute_error(ytest_p,ytest))
#print(kmodel.score(xtrain,ytrain))

"""NAIVE BAYES METHOD (MULTIMONIAL)"""

tr_acc = []
ts_acc = []
from sklearn.naive_bayes import MultinomialNB
mmodel = MultinomialNB()
for i in range(1,15): 
  xtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.3,random_state=i)
  mmodel.fit(xtrain, ytrain) 

  ytrain_p = mmodel.predict(xtrain)
  tr_acc.append((ytrain_p == ytrain).sum()/len(xtrain))

  ytest_p = mmodel.predict(xtest)
  ts_acc.append((ytest_p == ytest).sum()/len(xtest))

import matplotlib.pyplot as plt
plt.figure(figsize=(15,5))
plt.plot(range(1,15),tr_acc)
plt.plot(range(1,15),ts_acc,c='red')
plt.show()

from sklearn.naive_bayes import MultinomialNB
mmodel = MultinomialNB()


xtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.3,random_state=5)
mmodel.fit(xtrain, ytrain) 

ytrain_p = mmodel.predict(xtrain)
print((ytrain_p == ytrain).sum()/len(xtrain))

ytest_p = mmodel.predict(xtest)
print((ytest_p == ytest).sum()/len(xtest))

print(mean_absolute_error(ytrain_p,ytrain))
print(mean_absolute_error(ytest_p,ytest))

"""RANDOM FOREST CLASSIFICATION"""

tr_acc = []
ts_acc = []
from sklearn.ensemble import RandomForestClassifier
rmodel = RandomForestClassifier(max_depth=5, random_state=2)
for i in range(1,15): 
  xtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.3,random_state=i)
  rmodel.fit(xtrain, ytrain) 

  ytrain_p = rmodel.predict(xtrain)
  tr_acc.append((ytrain_p == ytrain).sum()/len(xtrain))

  ytest_p = rmodel.predict(xtest)
  ts_acc.append((ytest_p == ytest).sum()/len(xtest))

import matplotlib.pyplot as plt
plt.figure(figsize=(15,5))
plt.plot(range(1,15),tr_acc)
plt.plot(range(1,15),ts_acc,c='red')
plt.show()

#for i in range(1,15):
from sklearn.ensemble import RandomForestClassifier
rmodel = RandomForestClassifier(random_state=2, max_depth=5)

xtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.3,random_state=5)
rmodel.fit(xtrain, ytrain) 

ytrain_p = rmodel.predict(xtrain)
print((ytrain_p == ytrain).sum()/len(xtrain))

ytest_p = rmodel.predict(xtest)
print((ytest_p == ytest).sum()/len(xtest))

print(mean_absolute_error(ytrain_p,ytrain))
print(mean_absolute_error(ytest_p,ytest))

"""**ANALYSING THE ALGORITHMS**

1) PREDICTION %
"""

import numpy as np
import matplotlib.pyplot as plt

n_groups = 3
train_Pred = ( 80.12170385395537, 73.63083164300203, 79.20892494929006)
test_Pred = (79.649787032655 ,73.07146237576905, 78.65593942262187)

fig, ax = plt.subplots(figsize=(6,6))
index = np.arange(n_groups)
bar_width = 0.30
opacity = 0.8

rects1 = plt.bar(index, train_Pred, bar_width, alpha=opacity, color='r', label='TrainPrediction')

rects2 = plt.bar(index + bar_width, test_Pred, bar_width, alpha=opacity, color='g', label='TestPrediction')

plt.xlabel('---Algorithms---')
plt.ylabel('Prediction')
plt.ylim(50,90)
plt.title('Prediction % analysis')
plt.xticks(index + bar_width/2, ('KNN', 'Naive bayes', 'RandomForest'))
plt.legend()

plt.tight_layout()
plt.show()

"""2) ERROR %"""

n_groups = 3
train_Pred = ( 13.645841784989859, 0.26369168356997974, 13.60344827586207)
test_Pred = (15.469947941315665, 0.2692853762423095, 13.676762896355893)

fig, ax = plt.subplots(figsize=(7,4))
index = np.arange(n_groups)
bar_width = 0.30
opacity = 0.8

rects1 = plt.bar(index, train_Pred, bar_width, alpha=opacity, color='r', label='TrainError')

rects2 = plt.bar(index + bar_width, test_Pred, bar_width, alpha=opacity, color='g', label='TestError')

plt.xlabel('---Algorithms---')
plt.ylabel('Error analysis')
plt.ylim(0,20)
plt.title('Error % analysis')
plt.xticks(index + bar_width/2, ('KNN', 'Naive bayes', 'RandomForest'))
plt.legend()
plt.tight_layout()
plt.show()

